# 35.-Star-Wars-Reinforcement-Learning ‚≠êüî´
<p align="center"> 
  <img width="777" height="500" alt="image" src="https://github.com/user-attachments/assets/a5d5d50d-11c9-4830-abb4-a4ae46c4f4b2" />
</p>

A Machine Learning enthusiast who is also a Star Wars nerd - hence this project.

<br>

## Table of Contents
1. [Storyline](#storyline)
2. [What is Star Wars Reinforcement Learning?](#what)
3. [How to Run?](#run)
4. [Development Process](#development)
    1. [Phase 1 - Explore Planets](#phase1)
    2. [Phase 2 - War](#phase2)
    3. [Phase 3 - Hostile Planets and Hive Mind](#phase3)
    4. [Phase 4 - Multi-drones](#phase4)
    5. [Phase 5 - Bounty Hunter](#phase5) 
5. [Results](#results)
6. [Future Directions](#future)
7. [Spark any ideas?](#ideas)

<br>

## Storyline (for all of yall fellow nerds) <a name = "storyline"></a> 
<p align="center"> 
  <img width="777" height="500 alt="image" src="https://github.com/user-attachments/assets/10a028ea-bc02-4add-b706-5e501f264250" />
</p>
<p align="center"> 
  Generated by ChatGPT
</p>
You're a neutral planet explorer from Corusant, looking to discover planets to work with in the unexplored Outer Rim by sending out drones (the RL agents). You seek to find the best ways to explore as many planets as possible with a restricted amount of fuel in your drones to maximise efficiency.

Along the way, your drones encounter various obstacles and conflicts in the Outer Rim, but like any resilient explorer, you are determined to overcome them to achieve your objectives.

<br>

## What is Star Wars Reinforcement Learning? <a name = "what"></a> 
Finding the best policy for a Reinforcement Learning (RL) agent in a series of different RL Environments with the ultimate objective to explore as many planets as possible. 

This project is less on the RL algorithms itself, but more on learning how to design good RL Environments (i.e. design of reward functions, MDP, etc.). The RL Environments in this project are relatively simple, in a grid world style.

The RL Enviornments are all custom made [Gymnasium](https://gymnasium.farama.org/) RL Environments by me, designed as Partially Observable Markov Decision Processes (POMDP), each with their own state space, observation space and reward functions (their action spaces are the same). I used [Stable Baselines 3](https://stable-baselines.readthedocs.io/en/master/index.html)'s Proximal Policy Optimisation (PPO) as the RL algorithm for all the RL Environments.

**Technology stack used:**
- Farama Foundation's Gymnasium (https://gymnasium.farama.org/) ([OpenAI Gym's](https://www.gymlibrary.dev/) successor) (used to create the RL Environment)
- Stable Baselines 3 (https://stable-baselines.readthedocs.io/en/master/index.html) (used for quick implementation of DRL algorithms)
- Python

<br>

## How to Run? <a name = "run"></a> 
Each scenario is created as a seperate RL Environment, which I termed as a 'phase'. Each phase is stored in its respective folder. Play with the 'main.ipynb' file in each folder to try out the RL Environments!

<br>

## Development Process <a name = "development"></a> 
**I spend weeks watching pixels move around - honestly not good for my sanity.**

Generally, to get the RL agents to learn the best policy in each phase, I played with:
- Reward function tuning (adding new types of rewards, and adjusting their magnitude)
- Vectorising of the RL Environment and Normalisation of rewards
- RL algorithm hyperparaneter tuning

For each phase, I trained a 500k iterations PPO DRL algorithm and a 1M iterations PPO DRL algorithm. Try them out!

In this section, I will give a rough explanation of my thought processes in developing each phase, for more details (i.e. about the MDP) see each phase's 'main.ipynb'.

<br>

### Phase 1 - Explore Planets <a name = "phase1"></a>
*The Baseline RL Environment.*

Started simple with randomly generated planets in a fixed grid-world, giving the RL agent 4 discrete actions, and a reward function of just rewarding the RL agent if it visited a planet.

Quickly realised this was too difficult, as it took an extremely long training time. Hence, I gave the RL agent 'vision' and 'memory' to help it learn faster.

Did some additional fine tuning on the reward function since I noticed it kept getting stuck at corners, and staying at a fixed location.

<br>

### Phase 2 - War <a name = "phase2"></a>
*There's a war going on now.*

Started simple with a prey-predator situation, with the Separatists ships as the 'predator' and the Republic ships as the 'prey'. They have their own vision, and initially the Separatists ships will chase both the RL agent or the Republic ship in view, while the Republic ship will flee when a Separatist ship is in view. I later introduced more stochasticity in the Separatist and Republic ships' behaviour (see the 'npc_ships.py' file). 

They move at the same speed as the RL agent initially, but I quickly realised it took too long for the RL agent to learn since it was too difficult, hence I halved their speeds.

Simple reward function of, if a Separatists ship is adjacent to the RL agent, the RL agent will be penalised/takes damage.

I introduced Republic Ships in hopes of encouraging emergent behaviour (not reward-driven, since I don't reward the RL agent from being near a Republic ship, I merely allowed it to 'see' them in its vision) where the RL agent learns more complex policies like 'kiting' i.e. learns to camp near Republic ships so they will take agro from incoming Seperatist ships to maximise its rewards if the RL agent 'sees' a Republic ship. (unfortunately I was not able to show this, likely requires much more iterations to learn)

<br>

### Phase 3 - Hostile Planets and Hive Mind<a name = "phase3"></a>
*Even more chaos (hostile planets) to the war. Hive Mind feature involves implementing Meta-Epsiodes in the RL Environment.*

Started with randomly turning a portion of the planets into hostile planets, which emit a kill radius that kills any ship entering it. The RL agents does not know which plaents are hostile nor how big the kill radius are.

To help them, I introduced the idea of a 'Hive Mind', where initial RL agents randomly explore until they get killed, and the 'Hive Mind' remembers the death locations, visited planets, and seen areas of the map of previous RL agents. Within sub-episodes, the map is not reset. (except for the Separatist and Republic ships since I wanted to simulate the idea that by the time the planet explorers decide to send a new RL agent/drone, the state of the war could be different and new ships will occupy different parts of the map)

<br>

### Phase 4 - Multi-drones <a name = "phase4"></a>
*Implements Multi-agent collaboration in the RL Environment.*

-- Coming soon! --

- Inspired by this OpenAI article: https://openai.com/index/emergent-tool-use/

<br>

### Phase 5 - Bounty Hunter <a name = "phase5"></a> 
*Implements Adversarial agents competition in the RL Environment.*

-- Coming soon! --

- Inspired by the AI Warehouse Youtube channel: https://www.youtube.com/@aiwarehouse

<br>

## Results <a name = "results"></a> 


<br>

## Future Directions <a name = "future"></a> 
Application to real life - tackles the problem, with limited resources (fuel, determined by time of mission/episode), what is the best/optimal way to clear as many objectives (find as many planets) as possible?

My hypothesis:
Given limited fuel and a vast space, the RL agent could only explore a limited section of the vast space, hence it learns that policy (it somehow converged to only look at the left 60% region of the map) to find the most planets¬†possible

LLMs?

<br>

## Spark any ideas? <a name = "ideas"></a> 

Have an idea of a Star Wars themed RL Environment? Make it and let me know if you would like to collaborate!

For Phase 3, also another issue is that the RL agent found a policy to 'camp' at the last planet to farm the rewards off its positive rewards halo rather than just visiting it. It likely didnt want to visit it because once it visits it the planet will emit a negative rewards halo, which penalises it. 

Solution:
- I maybe could fix this by not making visited planets penalise the RL agent.
- or i could give a big reward for the RL agent if it finds all planets. However, I didnt want to do it because it dosent make sense as the RL agent isnt supposed to know how many planets there are in the map, and the objective is supposedly to try its best to find as many planets as possible.
- one more rationale is that we could still give the RL agent a big reward for finding all planets, if it has already seen all parts of the map (since if it has already seen all parts of the map, then it make sense that we should already know how many planets there are in the area and thus we should know if all planets has already been found)

As you could see, I believe there is alot of possible improvements and expansion on this project (could be applied to space travel or similar fields of work) to simulate different scenarios. So I open to anyone who willing to contribute new scenarios and the learned policies.

Currently the NPC ships, particularly the SeparatistShips are moving at half the speed of the RL agent, making it very easy for the RL agent to outmaneouvre them. I believe if we make them move at the same speed as the RL agent, it will force the RL agent to learn very different policies (and definitely will take longer/more iterations to train) (which I do not have the luxury of since Im running all this on my local laptop and each training process takes me 30 minutes)

But, I will open this phenomenon to your interpretation and future experiments.

Sorry the logs and saved RL models are really messy... I didnt manage to spend the time to organise them. But I did highlight the key RL models through which their policy/behaviour helped me to formulate the reward function to finding the best policy in that environment.

I didnt have the luxury to test the environments with other RL algorithms, but let me know if anyone did (though I assume they should more or less work similarly in finding the planets (since thats the ultimate goal in the various scenarios with slight differences in cumulative rewards)). Since in all my experiments i just used SB3's PPO only. 

talk about the future phase 4 and 5, and possibly using LLMs. Might do it when free 

Call if anyone wanna contribute (e.g. a new environment/better reward function for the existing ones, feel free to do a pull request)

Put extra render of the seperatist and republic logo, maybe planet logo also in the simulation



slow down the videos during demonstration, render the planets and ships with better UI and label the colouring what each of it mean

make a video collage of all the environments in a row (all 5)

should probably do reward cumulative graph as results... but I nvr focus on logging so I don't have... how I tell the algorithm is improving is solely by visually seeing the simulation (I know it's not the best way and seeing the reward convergence graph usually is the right way, but screw it, I'm just gonna stick to this cuz I no time study graphs and abit laze)

for the results section, unfortunately I didn't spend to time to keep the reward function consistent and for benchmarkin, so I can't really show you the improvements and convergence... but I can show you that these RL Environments works and gives RL agent the right signals to learn good/expected policies to achieve the objective (the videos serve as the resulting proof) (can't show reward numerical proof unfortunately) 
